---
title             : "The N400's 3 As: Association, Automaticity, Attenuation (and Some Semantics Too)"
shorttitle        : "N400 Association Attenuation"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes 
    address       : "325 Market Street, Harrisburg, PA, 17101"
    email         : "ebuchanan@harrisburgu.edu"
  - name          : "John E. Scofield"
    affiliation   : "2"
  - name          : "Nathan R. Nunley"
    affiliation   : "3"
  - name          : "Addie Wikowsky"
    affiliation   : "4"

affiliation:
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "2"
    institution   : "University of Missouri"
  - id            : "3"
    institution   : "University of Mississippi"
  - id            : "4"

author_note: |
  

abstract: >
  The N400 waveform carries new insight into the nature of linguistic processing and may shed light into the automaticity of priming word relationships. We investigated semantic and associative word pairs in classic lexical decision and letter search tasks to examine their differences in cognitive processing. Normed database information was used to create orthogonal semantic and associative word relationships to clearly define N400 waveforms and priming for these pairs. Participants showed N400 reduction for related word pairs, both semantic and associative, in comparison to unrelated word pairs for the lexical decision task, indicating automatic access for both types of relatedness. For a letter search task, the N400 showed differences between nonwords and other stimuli but no attenuation for related pairs. Response latency data indicated associative priming in both tasks with semantic priming also found in the letter search task. These results help discern possible automatic and controlled processes occurring during these tasks, as the N400 may show automatic processing during the lexical decision task, while the response latency data may provide evidence for controlled processing during the letter search task. 
  
keywords          : "association, semantics, priming, N400, EEG, lexical decision, letter search"

bibliography      : ["r-references.bib", "eeg refs.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r libraries, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
library("papaja")
options(scipen=999)
library(nlme)
library(reshape)
library(pracma)
library(ggplot2)
library(cowplot)
cleanup = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line.x = element_line(colour = "black"), 
              axis.line.y = element_line(colour = "black"), 
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 15),
              legend.position='none',
              plot.margin = unit(c(0.3,1,0.3,0.3),'cm'))
cleanup2 = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line.x = element_line(colour = "black"), 
              axis.line.y = element_line(colour = "black"), 
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 15))
```

```{r create-eeg-data, eval = FALSE, include = FALSE}
##find the files
list.filenames = list.files(path = "./brain_data")
##create empty list to store stuff
list.data = list()
##open all the files 
for (i in 1:length(list.filenames)){
  list.data[[i]]<-read.delim(paste("./brain_data/", list.filenames[i], sep = ""), header = FALSE, skip = 1)
  }
names(list.data) = list.filenames
library(data.table)
fulldata = rbindlist(list.data, fill = T)
fulldata = fulldata[ , -1152]
fulldata$partno = c(rep(substr(list.filenames[1:16], 0, 2), each = 30),
                    rep(substr(list.filenames[17:length(list.filenames)], 0, 2), each = 20))
fulldata$stim = c(rep(substr(list.filenames[1:16], 4, 4), each = 30),
                    rep(substr(list.filenames[17:length(list.filenames)], 4, 4), each = 20))

fulldata$task = c(rep(substr(list.filenames[1:16], 5, 5), each = 30),
                    rep(substr(list.filenames[17:length(list.filenames)], 5, 5), each = 20))


##only take five sites we need
levels(fulldata$V1)
sitedata = subset(fulldata, V1 == "CPZ" | V1 == "CZ" | V1 == "FCZ" | V1 == "FZ" | V1 == "PZ")
sitedata$V1 = droplevels(sitedata$V1)
table(sitedata$V1, sitedata$partno)
table(sitedata$stim, sitedata$task)
#no 64 for participant 2

##only take the stims + task we need, use target word
#stim 3 is NW, 4 is UR, 5 Assoc, 6 Sem
#task 0 is LST 1st, 2 is LST 2nd, 4 is LDT 1st, 6 is LDT 2nd
#remember that you originally labelled these the other way, but then the stimulus presentation was red LST paired with 0 and 2 and green LDT paired with 4 and 6
sitedata$stim = factor(sitedata$stim,
                       levels = 3:6,
                       labels = c("Nonword", "Unrelated", "Associative", "Semantic"))
finaldata = subset(sitedata, task == 2 | task == 6)

write.csv(finaldata, "final_eeg.csv", row.names = F)
```

Semantic facilitation through priming occurs when a related cue word speeds the processing of a following target word [@Meyer1971]. For example, if a person is reading about a yacht race, the word boat is easier to process because of previous activation in semantic memory. Research suggests that priming transpires by both automatic and controlled processes. The automatic model proposes that related words are linked in the brain due to overlapping features [@Collins1975]. Target words are activated without conscious control due to automatic spreading activation within related cognitive networks. Lexical and feature networks are thought to be stored separately, so that semantic priming is the activation from the feature network feeding back into the lexical level [@Stolz1996a]. The overlap of a second word's semantic relatedness makes word recognition easier because it, in essence, has already been processed. The controlled process model proposes that people actively use cognitive strategies to connect related words together. @Neely1991 describes both expectancy generation and post lexical matching as ways that target word processing may be speeded. In expectancy generation, people consciously attempt to predict the words and ideas that will appear next, especially in sentences. Whereas in post lexical matching, people delay processing of the second target word so that it can be compared to the cue word for evaluation. In both cases, the target word is quickened by its relationship to the cue word.
  
  Traditionally, priming has been tested with a simple word or nonword decision called a lexical decision task. Participants are shown a cue or priming word, followed by a related or unrelated target word for the word/nonword judgment. Priming occurs when the judgment for the target is speeded for related pairs over unrelated pairs. Lexical decision tasks have been criticized for their inability to distinguish between automatic and controlled processing, so both single presentation lexical decision tasks and masked priming manipulations  have been introduced to negate controlled processing [@Ford1983]. In a single lexical decision task, participants assess both the cue and target word so that they are not as overtly paired together. Experimenters might also mask or distort the cue word, so that participants do not believe they can perceive the cue word. Even though words are garbled, word perception occurs at a subliminal level and often facilitates the target word with automatic activation. 
  
##Priming in the Brain
  Event related potentials (ERPs) are used to distinguish both the nature of priming and the automaticity of priming. The use of ERPs is advantageous, measuring brain activity per an electroencephalogram (EEG) with good temporal resolution, and is thought to be a sensitive measure of real-time language processing [@Kutas2000]. The  N400 is a negative waveform that occurs 400 ms after the participant is presented with a stimulus [@Brown1993]. The N400 has been described as a 'contextual integration process', in which meanings of words are integrated and functions, bridging together sensory information and meaningful representations [@Kutas2000]. The amplitude of the N400 is sensitive to contextual word presentations, varying systematically with semantic processing. This change justifies the use of the N400 as an appropriate dependent measure for lexical decision tasks. When presented with related words, there is an attenuation of the N400, meaning a more positive waveform when compared to unrelated word presentation. This difference in waveforms indicates a lessened contextual integration process because word meanings are already activated.
  
Multiple theories of the N400, however, have been proposed and debated on what explicitly the N400 indexes. On one hand, processes associated with the N400 are believed to occur post-word recognition. @Brown1993 examined a lexical decision task paired with masked priming. No differences were found in the N400 wave between related and unrelated words in the masked prime condition. @Brown1993 concluded that this finding indicated that semantic activation was a controlled process, because attenuation only occurred when words were known. Thus, an 'integrating' process transpires with semantic information from multi-word characteristic representations [@Kutas2011; @Hagoort2009]. This condition supposedly rules out automatic processes, because the masked prime condition only allowed automatic processes to take place. Masked priming did not allow the participants to consciously name the prime words they had seen; thus, they were not able to purposefully employ conscious cognitive strategies in processing these words. However, @Deacon2000 have found that with shorter stimulus onset asynchronies (SOAs), this effect of masked priming disappears. SOAs are the time interval between the prime word presentation and the target word appearance. Short SOAs are thought to only allow for automatic processing because the controlled, attention based processing has not had time yet to occur. Their study showed the masked primes long enough to enhance priming, while remaining imperceptible. With these modifications, @Deacon2004 found equal N400 attenuation for the masked and unmasked primes. This result would indicate that automatic activation was taking place, as the masked prime condition did not allow controlled processes to take place. @Kiefer2002 has found similar results in the N400 using different masking levels, which kept judgment ability of prime words below chance. 
  
A separate theory suggests that N400 effects are seen pre-word recognition. The N400 was found to be sensitive to pseudo- or nonwords, even when absent a resemblance to real word counterparts. @Deacon2004 explain that this result could imply processes that precede word recognition, such as orthographic or phonological analysis. More recently, @Federmeier2009 suggested that the N400 indexes access to semantic memory. Meaningful stimuli representing a multitude of modalities indicates a sensitivity with attention, albeit still can occur in its absence. Processing from modalities can integrate, yielding different meanings from different contexts, respectively [@Federmeier2009]. Regardless of competing aspects as to what the N400 is estimated to index, vital insights have been made crossing different cognitive domains, with the N400 illuminating aspects originating from these different domains [@Kutas2011]. 
  
@Rolke2001 used the attention blink rapid serial visual presentation (RSVP) paradigm, in which participants identified target words within a stream of distractor words presented in a different color. By selecting items via specifying the row and column within a matrix, participants identified the target word they had previously seen. These studies compare to masked priming, and show automatic activation of semantic information even when targets were missed [@Rolke2001]. Letter search tasks also  reduce semantic priming by focusing attention on the lexical level instead of a feature meaning level [@Friedrich1991]. In this task, participants are asked to determine if cue and target words contain a specific letter presented. @Stolz1996a stipulate that this eliminated or reduced priming indicates non-automatic semantic priming. However, it is also important to note that @Tse2007 did yield evidence that letter search primes produced semantic priming for low-frequency targets, albeit not for high-frequency targets. In @Smith2001 letter search and lexical decision combined study, they found that the letter search task eliminated semantic priming when compared to unrelated word pairs and the lexical decision task. Yet, @Mari-Beffa2005 found ERP evidence for semantic processing of the prime word during letter search tasks with the attenuation of the N400.
  
##Association

From a theoretical standpoint, the relation between associative and semantic processing follows a deep line of research. Associative word pairs are words that are linked in one's memory by contextual relationships, such as basket and picnic [@Nelson2004]. Another example would be a word pair like alien and predator, which would be associatively linked for Americans due to the movies and popular culture. Semantic word pairs are those linked by their shared features and meaning, such as *wasp* and *bee* [@Buchanan2013; @McRae2005; @Vinson2008].
  
Associative and semantic relationships between words are experimentally definable by the use of normed databases. @Maki2004 took the online dictionary, WordNet [@Felbaum1998], and used software by @Patwardhan2003 to create a database of words displaying the semantic distance between individual words. This database displays the relatedness between two words by measuring how semantically close words appear in hierarchy, or the JCN [@Jiang1997]. JCN measures the word pairs' informational distance from one another, or their semantic similarities. Therefore, a low JCN score demonstrates a close semantic relationship. Additionally, we can use a measure of semantic feature overlap to examine the semantic relatedness between word pairs [@Buchanan2013; @McRae2005; @Vinson2008], and this measure is factorally related to JCN as a semantic measure [@Maki2008]. Another useful database, created by @Nelson2004, is centered on the associative relationships between words. Participants were given cue words and asked to write the first word that came to mind. These responses were asked of and averaged over many participants. The probability of a cue word eliciting the target word is called the forward strength (FSG). For example, when participants are shown the word *lost*, the most common response is *found*, which has a FSG of .75 or occurs about 75% of the time. 
  
##Separating Semantic and Associative Priming

A meta-analytic review from @Lucas2000 examined semantic priming in the absence of association. Effect sizes for semantic priming alone were lower than associative priming. However, with the addition of an associative relationship to an existing semantic relationship, priming effects nearly doubled, also known as the associative boost [@Moss1995]. This result suggests that semantic relationships, that concurrently have associations, can increase priming effects. Priming effects, therefore, are suggested not to be based on association in isolation. @Hutchison2003 argues against Lucas, suggesting positive evidence for associative priming. Automatic priming was sensitive to associative strength as well as feature overlap. These points of contention provide impetus for more research centering on distinctions between associative and semantic priming.  
  
With the databases described above, orthogonal word pair stimuli can be created to examine associative and semantic priming individually and indeed, priming can be found for each relation separately [@Buchanan2010]. Few studies have directly compared associative and semantic relationships, especially focusing on the brain. @Deacon2004 claim that hemispheric differences exist in lexico-semantic representation, comparing associative and semantic priming. Deacon et al. concluded that semantic features are localized in the right hemisphere, whereas association is localized more within the left hemisphere of the brain. The current study, with an aim to elaborate on basic theoretical questions such as the relationship between associative and semantic processing, examined the relationship between N400 activation, priming task, and word relationship type. Participants were given both a single lexical decision and letter search task, along with separate semantic, associative, and unrelated word pairs. We expected that the N400 modulation might vary from the different types of word relation, which would indicate differences in cognitive processing and word organization.
  
#Method

##Participants

Twenty undergraduate students were recruited from the University of Mississippi (thirteen women and seven men), and all volunteered to participate. All participants were English speakers. The experiment was carried out with the permission of the University's Institutional Review Board, and all participants signed corresponding consent forms. One participant's EEG data was corrupted and could not be used, and another participant was excluded for poor task performance (below chance), leaving eighteen participants (twelve women and six men).
  
##EEG Acquisition

The system used was a 32 Channel EEG Quik-Cap (https://compumedicsneuroscan.com/product/32-channels-quik-cap/) connected to a NuAmps monopolar digital amplifier (https://compumedicsneuroscan.com/product/nuamps-40-channel-eeg-erp-amplifier/), which was connected to a computer running SCAN 4.5 software to record the data. The Quik-Cap includes Ag/AgCl sintered ring electrodes and the layout follows the International 10/20 system with the following sites: F7, FT7, T7, TP7, P7, FP1, F3, FC3, C3, CP3, P3, O1, Fz, FCz, Cz, CPz, Pz, Oz, FP2, F4, FC4, C4, CP4, P4, O2, F8, FT8, T8, TP8, and P8. The ground electrode was embedded in the cab at FPz, and two elecctrodes were attached to the mastoid bones. 

The SCAN software was capable of managing continuous digital data captured by the NuAmps amplifier. STIM2 was used to coordinate the timing issues associated with Windows operating system and collecting EEG data on a separate computer (https://compumedicsneuroscan.com/product/stim2-precise-stimulus-presentation/). STIM2 also served as the software base for programming and operating experiments of this nature. The sensors in the EEG cap were sponges injected with 130 ml of electrically conductive solution (non-toxic and non-irritating). Also, to protect the participants and equipment, a surge protector was used at all times during data acquisition. The sensors recorded electrical activity just below the scalp, displaying brain activation. This data was amplified by the NuAmps hardware, and processed and recorded continuously by the SCAN software. At the start of th experiment, adjustments were made until impedance values were below 5 k$\Omega$. Data processing is described below. 
  
##Materials

This experiment consisted of 360 word pairs separated into levels in which the target words were unrelated to the prime (120), semantically associated to the prime (60), associatively related to the prime (60), or were nonwords (120). We used only a small number of related word pairs to try to reduce expectancy effects described in the introduction [@Neely1991]. These 360 pairs were split evenly between the lexical decision and letter search task, therefore, each task contained 60 unrelated pairs, 30 semantically related pairs, 30 associatively related pairs, and 60 nonword pairings. The ratio of yes/no correct answers for words and nonwords in the lexical decision task was 2:1 and 1:1 yes/no decisions in the letter search task. Splitting the nonword pairs over both the letter search and lexical decision task created a higher yes/no ratio for the lexical decision task, which was controlled for by mixing both tasks together.

```{r stimuli-task-info, include = FALSE}
task = read.csv("task_data.csv")

meansFSG = tapply(task$fsg, list(task$task, task$type), mean, na.rm = T)
sdsFSG = tapply(task$fsg, list(task$task, task$type), sd, na.rm = T)

meansJCN = tapply(task$jcn, list(task$task, task$type), mean, na.rm = T)
sdsJCN = tapply(task$jcn, list(task$task, task$type), sd, na.rm = T)
```

The stimuli were selected from the @Nelson2004 associative word norms and @Maki2004 semantic word norms. The associative word pairs were chosen using the criteria that they were highly associatively related, having an FSG score greater than .50; with little or no semantic similarities, determined by having a JCN score of greater than 20. An example of an associative pair would be *dairy-cow*. The semantic word pairs were chosen using the criteria that they had a high semantic relatedness shown in a JCN of 3 or less; and were not associatively related, having an FSG of less than .01 (e.g., *inn-lodge*). For associative word pairs, the mean FSG was *M* = `r printnum(meansFSG[1,1], gt1 = F)` (*SD* = `r printnum(sdsFSG[1,1], gt1 = F)`) for the LDT, and *M* = `r printnum(meansFSG[2,1], gt1 = F)` (*SD* = `r printnum(sdsFSG[2,1], gt1 = F)`) for the LST. The JCN was high for associative pairs, LDT *M* = `r printnum(meansJCN[1,1])` (*SD* = `r printnum(sdsJCN[1,1])`) and LST *M* = `r printnum(meansJCN[2,1])` (*SD* = `r printnum(sdsJCN[2,1])`). For semantic pairs, the JCN was low for both the LDT, *M* = `r printnum(meansJCN[1,2])` (*SD* = `r printnum(sdsJCN[1,2])`), and LST, *M* = `r printnum(meansJCN[2,2])` (*SD* = `r printnum(sdsJCN[2,2])`). The FSG was kept low for the semantic pairs, LDT, *M* = `r printnum(meansFSG[1,2], gt1 = F)` (*SD* = `r printnum(sdsFSG[1,2], gt1 = F)`), and LST, *M* = `r printnum(meansFSG[2,2], gt1 = F)` (*SD* = `r printnum(sdsFSG[2,2], gt1 = F)`).

The unrelated words were chosen so that they had no similarities (were unpaired in the databases), such as *blender* and *compass.* For nonword pairs, the target word had one letter changed so that it no longer represented a real word, yet the structure was left intact to require that the participant process the word cognitively. Essentially, nonwords were orthographically similar to its real word counterpart, except for the change in a single letter. For example, the word *pond* can be changed to *pund* to produce a nonword target. All materials and their database values can be found at our Open Science Foundation page: https://osf.io/h5sd6/, along with the markdown template used to create this paper [@R-papaja]. 

##Procedure

Testing occurred in one session consisting of six blocks of acquired data, broken up by brief rest periods. Before each participant was measured, the system was configured to the correct settings, and the hardware prepared. Two reference channels, which define zero voltage, were placed on the right and left mastoid bones. 
  
We modeled the current task after @Smith2001 lexical decision and letter search task combination. @Smith2001 used a choice task procedure, where the color of the target word indicated the target task. One color denoted lexical decision with another color denoting letter search. The lexical decision task involved participants observing a word onscreen and deciding whether or not it was a word or nonword (such as *tortoise* and *werm*). Nonrelated word pairs were created by taking prime and target words from related pairs and randomly rearranging them to eliminate relationships between primes and targets. The letter search task involved participants observing a word onscreen and deciding whether it contained a repeated letter or not (i.e., the repeated letters in *doctor* versus no repeated letters in *nurse*). Words were presented onscreen, and would stay there until the participant pressed the corresponding keys for yes and no. Participant responses were time limited and truncated to 60 seconds. The 1 and 9 keys were used on the number row of the keyboard, in the participant's lap to help eliminate muscle movement artifact in the data. 
  
Participants were first given instructions on how to perform the lexical decision task, followed by 15 practice trials. Next, they were given instructions on how to judge the letter search task, followed by 15 practice trials. Participants were then given a practice session with both letter search and lexical decision trials mixed together. Trials were color coded for the type of decision participants had to complete (i.e., letter search was red, while lexical decision was green). The experiment made use of six sets of 60 randomly assigned word pairs for a total of 360 trials. These trials were presented in Arial 19-point font, and the inter-trial interval was set to two  seconds to allow complete recording of the N400 waveform. Trials were recorded in five minute blocks, and between blocks participants were allowed to rest to prevent fatigue. The current task differed from @Smith2001 in that participants responded to every word (prime and targets), instead of only targets. Therefore, there was no typical fixed stimulus onset asynchrony (SOA) because participant responses were self-paced. 
  
#Results

##N400 Waveform Analysis

###EEG Preparation

The data were cleared of artifact data using EEGLAB [http://sccn.ucsd.edu/eeglab/; @Delorme2004], an open source MATLAB tool for processing electrophysiological data. The program automatically scanned for and removed artifacts caused by eye-blinking using independent component analysis. Next, the datasets were visually inspected and any remaining corrupted sections were removed manually. Ninety percent of the data was retained across all trials and stimulus types after muscular artifact data were removed. However, a loss rate of 20-30 percent is not uncommon, especially with older EEG systems. The data were combined by task and stimulus type exclusively for the second word in each pair. Five sites were chosen to examine priming for nonwords, associative and semantic word pairs based on a survey of the literature. Fz, FCz, Cz, CPz, and Pz were used from the midline. Oz was excluded due to equipment problems across all participants. Using MATLAB, the N400 area under the curve was calculated for each electrode site, stimulus, and task (averaging over trials) 300-500 ms after stimuli presentation. A constant score was subtracted from all EEG points to ensure all curves were below zero for area under the curve calculations.  

```{r create-area-peak, include = FALSE}
finaldata = read.csv("final_eeg.csv")
finaldata$task = factor(finaldata$task,
                        levels = c(2,6),
                        labels = c("LST", "LDT"))
#put site at the end
colnames(finaldata)[1] = "site"
finaldata = finaldata[ , c(2:ncol(finaldata), 1)]

#columns are -150 ms to 999 ms around stimulus presentation
#find peaks
finaldata$n4peak = apply(finaldata[ , 450:650], 1, min)
tapply(finaldata$n4peak, list(finaldata$stim, finaldata$task), mean)
#find where peak
finaldata$n4peakW = apply(finaldata[ , 450:650], 1, which.min) + 449
tapply(finaldata$n4peakW, list(finaldata$stim, finaldata$task), mean)

#calculate from 450 to 650, which is 300 to 500
#subtract a constant so they are all negative
largest = apply(finaldata[ , 450:650], 1, max)
largestvalue = max(largest)
finaldata$area = apply(finaldata[ , 450:650], 1, function(x){trapz(300:500, as.numeric((x-largestvalue)))})
tapply(finaldata$area, list(finaldata$stim, finaldata$task), mean)
```

```{r area-DS, include=FALSE}
##import data
longdata <- finaldata[ , c("site" , "stim", "task", "partno", "area" )]
##Non-Recursive Procedure with Moving Criterion
## 15 = 2.326  20 = 2.391  average equals 2.3585
colnames(longdata) = c("site","type","task","partno", "curve")

table(longdata$type)
longdata$type = factor(longdata$type, 
                       levels = c("unrelated", "nonword", "semantic", "associative"),
                       labels = c("Unrelated", "Nonword", "Semantic", "Associative"))
##non recursive outliers
zscore = unsplit(lapply(split(longdata$curve, list(longdata$partno, longdata$task)),
                        scale),list(longdata$partno, longdata$task))
summary(abs(zscore) < 2.3585) ##three outliers
noout = subset(longdata, abs(zscore) < 2.3585) ##exclude outliers

##other data screening
random = rchisq(nrow(noout), 7)
fake = lm(random~., data=noout[, -4])
standardized = rstudent(fake)

#linear
{qqnorm(standardized)
abline(0,1)}

#normal
hist(standardized, breaks=15)

##homogeneity and homoscedaticity
fitvalues = scale(fake$fitted.values)
{plot(fitvalues, standardized) 
abline(0,0)
abline(v = 0)}

```

###Data Screening

@VanSelst1994 describe that outlier elimination procedures can be affected by factors such as sample size or data skewness. They, as well as @Miller1991, describe procedures for adaptive outlier criteria based on sample size to correct for this any bias due to sample size. We utilized a non-recursive procedure with a moving criterion for outlier elimination. For example, traditional outlier identification may be based on a *z*-score criteria of two or more standard deviations away from the mean score. In the @VanSelst1994, this cut-off *z*-score is adjusted by sample size, and therefore, we used the average of their recommendations for 15 to 20 participants, $z_{critical}$ = 2.36. The non-recursive procedure involves only examing the data once for outliers, rather than continuing to screen for outliers iteratively until no outliers remained. Across 18 participants by five sites, three outlying data points were identified and subsequently removed from further analysis. Data were also screened for parametric assumptions of linearity, normality, homogeneity, and homoscedasticity. The data were slightly negatively skewed, but with the large quantity of data for each participant as well as the choice of analysis, test statistics should be robust to this slight skew.

###Data Analytic Plan

To analyze this data, we used multilevel models (MLM) to control for correlated error due to repeated measures of sites and stimulus type for each participant [@Gelman2006]. These models were calculated using the *nlme* package in *R* [@Pinheiro2017]. First, a model with only the intercept was compared to a model with participants as a random intercept factor. Random intercepts allow each participant to have different average scores for areas under the curve or peak latency (see below). If the random intercept model was better than the intercept only model, then all forthcoming models would include participants as a random intercept factor. Models were compared only to the previous step and were deamed "significant" if the likelihood ratio difference score, $\Delta\chi^2$ was greater than to be expected given the change in degrees of freedom between models. Therefore, the *p*-values for each $\Delta\chi^2$ were calculated based on $\Delta df$, and $\alpha$ was set to .05. The two tasks, lexical desicion and letter search, were analyzed in separate models with the area under the curve as the dependent variable. The independent variables included the dummy coded site location as a control variable, followed by stimulus type coded as a dummy variable. In this analysis, we wished to compare each stimulus type to every other stimulus type, and therefore, we set $\alpha$ for these six comparisons to .05/6 = .008. The stimuli variable was recoded to examine all pairwise comparisons. 

###Area Under the Curve Results 

Table \@ref(tab:area-table-model) includes the model statistics for the lexical decision and letter search tasks examining area under the curve. Participants were included as a random intercept factor, as this model was significantly better than an intercept only model, *p* < .001. The addition of the predictors of site and type of stimulus were also significant for both models, *p* < .001 and *p* = .003. Table \@ref(tab:area-table-est) includes the estimates for each pairwise comparison for word stimulus type. For the lexical decision task, we found that nonwords and unrelated had significantly larger areas under the curve than related word pairs. Nonwords and unrelated pairs were not different using our corrected $\alpha$ value. This find replicated previous work that the N400 was larger for unexpected words, while related word pairs showed attenuation. Semantic and associative stimuli did not show differences in their area under the curve. Figure \@ref(fig:graph-LDT) displays the ERP waveforms, separated by site, for the lexical decision task. For the letter search task, a similar pattern emerges for nonwords, in that they showed larger areas under the curve than all other stimuli. However, we did not find attenuation for related words, as unrelated, semantic, and associative words showed the same area under the curve in this task. Figure \@ref(fig:graph-LST) portrays the letter search task. The two gray lines represent unrelated and nonwords, which have larger areas under the curve than the two black lines, which represent semantic and associative word pairs. 

```{r area-LDT, include = FALSE}
##split on task
LDT = subset(noout, task == "LDT")

####ldt models####
LDTmodel1 = gls(curve ~ 1, 
                data = LDT, method = "ML", 
                na.action = "na.omit")

LDTmodel2 = lme(curve ~ 1, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LDTmodel3 = lme(curve ~ site + type, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LDToverall = anova(LDTmodel1, LDTmodel2, LDTmodel3)

levels(LDT$type)
LDT$type2 = factor(LDT$type,
                  levels = c("Nonword", "Unrelated", "Semantic", "Associative"))

LDTmodel3.1 = lme(curve ~ site + type2, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LDT$type3 = factor(LDT$type,
                  levels = c("Semantic", "Nonword", "Unrelated", "Associative"))

LDTmodel3.2 = lme(curve ~ site + type3, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

```

```{r graph-LDT, echo=FALSE, fig.cap = "EEG Waveforms for five midline sites for the four experimental stimuli in the lexical decision task.", fig.height=8, fig.width=8}

#overall data
#need to average down by participant
graph_data = finaldata[ , c(1152, 1153, 1154, 151:850) ]
colnames(graph_data)[1:3] = c('Stimuli','Task','Site')

graph_data = aggregate(graph_data[, 4:ncol(graph_data)], 
          list(graph_data$Stimuli, graph_data$Task, graph_data$Site), mean)
colnames(graph_data)[1:3] = c('Stimuli','Task','Site')
longdata = melt(graph_data,
                   id = c('Task','Stimuli','Site'))
longdata$variable = as.numeric(longdata$variable)

#individual sites data 
ldt_dat = subset(longdata, Task == 'LDT')
lst_dat = subset(longdata, Task == 'LST')
cpz_ldt = subset(ldt_dat, Site == 'CPZ')
cz_ldt = subset(ldt_dat, Site == 'CZ')
fcz_ldt = subset(ldt_dat, Site == 'FCZ')
fz_ldt = subset(ldt_dat, Site == 'FZ')
pz_ldt = subset(ldt_dat, Site == 'PZ')
cpz_lst = subset(lst_dat, Site == 'CPZ')
cz_lst = subset(lst_dat, Site == 'CZ')
fcz_lst = subset(lst_dat, Site == 'FCZ')
fz_lst = subset(lst_dat, Site == 'FZ')
pz_lst = subset(lst_dat, Site == 'PZ')

##ldt graphs
cpz_ldt_g = ggplot(data=cpz_ldt, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'CPz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
cz_ldt_g = ggplot(data=cz_ldt, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'Cz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
fcz_ldt_g = ggplot(data=fcz_ldt, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'FCz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
fz_ldt_g = ggplot(data=fz_ldt, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'Fz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
pz_ldt_g = ggplot(data=pz_ldt, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'Pz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
temp = ggplot(data=pz_ldt, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid"),
                        labels = c("Associative", "Nonword", "Semantic", "Unrelated"),
                        name = "Stimuli Type") +
  scale_color_manual(values=c('black','gray48','black','gray60'),
                     labels = c("Associative", "Nonword", "Semantic", "Unrelated"),
                     name = "Stimuli Type") 
mylegend <- get_legend(temp)
ldt_grid = plot_grid(cpz_ldt_g,cz_ldt_g,fcz_ldt_g,
        fz_ldt_g,pz_ldt_g,ncol=2)
ldt_plot = ldt_grid + draw_grob(mylegend, 2/3,0,1/3,0.4)
ldt_plot
```

```{r area-LST, include = FALSE}
##split on task

LST = subset(noout, task == "LST")

####LST models####
LSTmodel1 = gls(curve ~ 1, 
                data = LST, method = "ML", 
                na.action = "na.omit")

LSTmodel2 = lme(curve ~ 1, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LSTmodel3 = lme(curve ~ site + type, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LSToverall = anova(LSTmodel1, LSTmodel2, LSTmodel3)

levels(LST$type)
LST$type2 = factor(LST$type,
                  levels = c("Nonword", "Unrelated", "Semantic", "Associative"))

LSTmodel3.1 = lme(curve ~ site + type2, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LST$type3 = factor(LST$type,
                  levels = c("Semantic", "Nonword", "Unrelated", "Associative"))

LSTmodel3.2 = lme(curve ~ site + type3, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)
```

```{r graph-LST, echo=FALSE,  fig.cap = "EEG Waveforms for the letter search task across the five midline sites for each type of stimuli.", fig.height=8, fig.width=8}
#lst graphs
cpz_lst_g = ggplot(data=cpz_lst, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'CPz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
cz_lst_g = ggplot(data=cz_lst, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'Cz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
fcz_lst_g = ggplot(data=fcz_lst, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'FCz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
fz_lst_g = ggplot(data=fz_lst, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'Fz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
pz_lst_g = ggplot(data=pz_lst, aes(x=variable, y = value, group = Stimuli)) + 
  geom_line(aes(color=Stimuli,linetype=Stimuli), size=1) +
  scale_linetype_manual(values=c("solid","22","22","solid")) +
  scale_color_manual(values=c('black','gray48','black','gray60')) +
  scale_y_continuous(name = 'Amplitude', limits = c(-2,6)) +
  scale_x_discrete(name = 'Pz', limits = c(175,350,525,700)) +
  geom_hline(yintercept = 0) +
  cleanup
lst_grid = plot_grid(cpz_lst_g,cz_lst_g,fcz_lst_g,
                     fz_lst_g,pz_lst_g,ncol=2)
lst_plot = lst_grid + draw_grob(mylegend, 2/3,0,1/3,0.4)

lst_plot
```

```{r area-table-model, results='asis', echo = FALSE}
tableprint = matrix(NA, nrow = 6, ncol = 7)
tableprint[ , 1] = c("LDT Intercept", "LDT Random Intercept", "LDT Full",
                     "LST Intercept", "LST Random Intercept", "LST Full")
tableprint[ , 2] = c(LDToverall$df, LSToverall$df)
tableprint[ , 3] = c(LDToverall$AIC, LSToverall$AIC)
tableprint[ , 4] = c(LDToverall$BIC, LSToverall$BIC)
tableprint[ , 5] = c(LDToverall$logLik, LSToverall$logLik)
tableprint[ , 6] = c(LDToverall$L.Ratio, LSToverall$L.Ratio)
tableprint[ , 7] = c(LDToverall$`p-value`, LSToverall$`p-value`)

tableprint[ , 3:6] = printnum(as.numeric(tableprint[ , 3:6]), 
                              gt1 = TRUE, zero = TRUE, big.mark="")
tableprint[ , 7] = printnum(as.numeric(tableprint[ , 7]), 
                              gt1 = FALSE, zero = FALSE, digits = 3)

apa_table(as.data.frame(tableprint), 
          align = c("l", rep("c", 6)), 
          caption = "Area under curve model statistics",
          note = "AIC: Aikaike Information Criterion, BIC: Bayesian Information Criterion",
          format = "latex",
          escape = FALSE,
          col.names = c("Model", "$df$", "AIC", "BIC", "$\\chi^2$", "$\\Delta\\chi^2$", "$p$"))
```

```{r area-table-est, results='asis', echo = FALSE}
tableprint = matrix(NA, nrow = 20, ncol = 6)
tableprint[ , 1] = c(rep("LDT", 10), rep("LST", 10)) 
tableprint[ , 2] = rep(c("CZ", "FCZ", "FZ", "PZ", "Unrelated - Nonword", 
                     "Unrelated - Semantic", "Unrelated - Associative",
                     "Nonword - Semantic", "Nonword - Associative", 
                     "Semantic - Associative"), 2)
tableprint[1:7 , 3:6] = summary(LDTmodel3)$tTable[-1 , -3]
tableprint[8:9 , 3:6] = summary(LDTmodel3.1)$tTable[7:8 , -3]
tableprint[10 , 3:6] = summary(LDTmodel3.2)$tTable[8 , -3]

tableprint[11:17 , 3:6] = summary(LSTmodel3)$tTable[-1 , -3]
tableprint[18:19 , 3:6] = summary(LSTmodel3.1)$tTable[7:8 , -3]
tableprint[20 , 3:6] = summary(LSTmodel3.2)$tTable[8 , -3]

tableprint[ , 3:5] = printnum(as.numeric(tableprint[ , 3:5]), 
                              gt1 = TRUE, zero = TRUE)
tableprint[ , 6] = printnum(as.numeric(tableprint[ , 6]), 
                              gt1 = FALSE, zero = FALSE, digits = 3)

apa_table(tableprint, 
          align = c("l", rep("c", 6)), 
          caption = "Area under curve model estimates",
          escape = FALSE,
          note = "The site control level was considered CPZ. Degrees of freedom are 335 for lexical decision tasks and 332 for letter search tasks.",
          col.names = c("Task", "Predictor", "$b$", "$SE$", "$t$", "$p$"))

```

##Task Performance

```{r task-data-setup, include = FALSE}
#create a correctness score
task$correct = as.numeric(task$rightanswer == task$answer)
tapply(task$correct, list(task$type, task$task), mean)
means_correct = tapply(task$correct, list(task$task), mean)
sds_correct = tapply(task$correct, list(task$task), sd)
```

###Data Screening

One persons data was corrupt for the complete task component, and one participant's task excluded several of their responses. The missing responses were excluded for this analysis (*n* = 16 complete with 360 responses, *n* = 1 with 329 responses). Task data were scored for correctness in the two tasks, and overall performance was around 94% for each task: LDT, *M* = `r printnum(means_correct[1]*100)` (*SD* = `r printnum(sds_correct[1]*100)`) and LST, *M* = `r printnum(means_correct[1]*100)` (*SD* = `r printnum(sds_correct[1]*100)`). Incorrect trials (*n* = 335) were discarded for the response latency analysis. An analysis of outliers indicated there were 214 trials with long response latencies, and they were excluded from the analysis.

###Response Latency Results

Two MLM analyses were conducted on each task separately, with stimuli as the independent variable and response latency as the dependent variable, controlling for participants as a random factor (see Table \@ref(tab:RT-table-model)). In both the lexical decision and letter search tasks, there were significant improvements in the model by including stimuli as a predictor over the random intercept model, *p*s < .001. Each stimuli type was compared pairwise, and $\alpha$ was again set at .008 to control the Type I error rate. Table \@ref(tab:RT-table-est) includes these comparisons from the dummy coded models, and means with 95% confidence intervals are displayed in Figure \@ref(fig:RT-graph). For the lexical decision task, nonwords were slower than all other stimuli types. Unrelated words were not different from semantic word pairs, but were slower than associative word pairs. This finding indicated that the lexical decision task showed associative priming, but not semantic priming; however, there were not response latency differences for these two related word pair types. In contrast to this finding, and results from the N400 area under the curve, we found priming for both semantic and associative word pairs in the letter search task. Nonwords were again slower than all other stimuli types, followed by unrelated word pairs. Again, semantic and associative pairs were not different. These analyses were examined with the outliers included in the analysis, as we considered that eliminating 214 trials may have skewed the results. The pattern of results did not change, but the differences between unrelated pairs and other stimuli types do become larger. 

```{r RT-DS, include=FALSE}
##import data
longdata = task[ , c("task", "type", "partno", "rt", "correct")]
##Non-Recursive Procedure with Moving Criterion
## 15 = 2.326  20 = 2.391  average equals 2.3585

colnames(longdata) = c("task","type","partno","RT", "correct")
longdata = subset(longdata, correct == 1)

longdata$type=factor(longdata$type,
                     levels = c("UR","NW","LH","HL"),
                     labels = c("Unrelated", "Nonword", "Semantic", "Associative"))

##non recursive outliers
zscore = unsplit(lapply(split(longdata$RT, list(longdata$partno, longdata$task)),
                        scale),list(longdata$partno, longdata$task))

summary(abs(zscore) < 2.3585) ##see how many outliers
noout = subset(longdata, abs(zscore) < 2.3585) ##exclude outliers
tapply(noout$RT, list(noout$task, noout$type), mean)
```

```{r RT-LDT, include = FALSE}
##split on task
LDT = subset(noout, task == "LDT")

####ldt models####
LDTmodel1 = gls(RT ~ 1, 
                data = LDT, method = "ML", 
                na.action = "na.omit")

LDTmodel2 = lme(RT ~ 1, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LDTmodel3 = lme(RT ~ type, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LDToverall = anova(LDTmodel1, LDTmodel2, LDTmodel3)

levels(LDT$type)
LDT$type2 = factor(LDT$type,
                  levels = c("Nonword", "Unrelated", "Semantic", "Associative"))

LDTmodel3.1 = lme(RT ~ type2, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LDT$type3 = factor(LDT$type,
                  levels = c("Semantic", "Nonword", "Unrelated", "Associative"))

LDTmodel3.2 = lme(RT ~ type3, 
                data = LDT, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)
```

```{r RT-LST, include = FALSE}
##split on task
LST = subset(noout, task == "LST")

####LST models####
LSTmodel1 = gls(RT ~ 1, 
                data = LST, method = "ML", 
                na.action = "na.omit")

LSTmodel2 = lme(RT ~ 1, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LSTmodel3 = lme(RT ~ type, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LSToverall = anova(LSTmodel1, LSTmodel2, LSTmodel3)

levels(LST$type)
LST$type2 = factor(LST$type,
                  levels = c("Nonword", "Unrelated", "Semantic", "Associative"))

LSTmodel3.1 = lme(RT ~ type2, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

LST$type3 = factor(LST$type,
                  levels = c("Semantic", "Nonword", "Unrelated", "Associative"))

LSTmodel3.2 = lme(RT ~ type3, 
                data = LST, method = "ML", 
                na.action = "na.omit",
                random = ~1|partno)

```

```{r RT-table-model, results='asis', echo = FALSE}
tableprint = matrix(NA, nrow = 6, ncol = 7)
tableprint[ , 1] = c("LDT Intercept", "LDT Random Intercept", "LDT Full",
                     "LST Intercept", "LST Random Intercept", "LST Full")
tableprint[ , 2] = c(LDToverall$df, LSToverall$df)
tableprint[ , 3] = c(LDToverall$AIC, LSToverall$AIC)
tableprint[ , 4] = c(LDToverall$BIC, LSToverall$BIC)
tableprint[ , 5] = c(LDToverall$logLik, LSToverall$logLik)
tableprint[ , 6] = c(LDToverall$L.Ratio, LSToverall$L.Ratio)
tableprint[ , 7] = c(LDToverall$`p-value`, LSToverall$`p-value`)

tableprint[ , 3:6] = printnum(as.numeric(tableprint[ , 3:6]), 
                              gt1 = TRUE, zero = TRUE, big.mark="")
tableprint[ , 7] = printnum(as.numeric(tableprint[ , 7]), 
                              gt1 = FALSE, zero = FALSE, digits = 3)

apa_table(as.data.frame(tableprint), 
          align = c("l", rep("c", 6)), 
          caption = "Response latency model statistics",
          note = "AIC: Aikaike Information Criterion, BIC: Bayesian Information Criterion",
          format = "latex",
          escape = FALSE,
          col.names = c("Model", "$df$", "AIC", "BIC", "$\\chi^2$", "$\\Delta\\chi^2$", "$p$"))
```

```{r RT-table-est, results='asis', echo = FALSE}
tableprint = matrix(NA, nrow = 12, ncol = 6)
tableprint[ , 1] = c(rep("LDT", 6), rep("LST", 6)) 
tableprint[ , 2] = rep(c("Unrelated - Nonword", 
                     "Unrelated - Semantic", "Unrelated - Associative",
                     "Nonword - Semantic", "Nonword - Associative", 
                     "Semantic - Associative"), 2)
tableprint[1:3 , 3:6] = summary(LDTmodel3)$tTable[-1 , -3]
tableprint[4:5 , 3:6] = summary(LDTmodel3.1)$tTable[3:4 , -3]
tableprint[6 , 3:6] = summary(LDTmodel3.2)$tTable[4 , -3]

tableprint[7:9 , 3:6] = summary(LSTmodel3)$tTable[-1 , -3]
tableprint[10:11 , 3:6] = summary(LSTmodel3.1)$tTable[3:4 , -3]
tableprint[12 , 3:6] = summary(LSTmodel3.2)$tTable[4 , -3]

tableprint[ , 3:5] = printnum(as.numeric(tableprint[ , 3:5]), 
                              gt1 = TRUE, zero = TRUE)
tableprint[ , 6] = printnum(as.numeric(tableprint[ , 6]), 
                              gt1 = FALSE, zero = FALSE, digits = 3)

apa_table(tableprint, 
          align = c("l", rep("c", 6)), 
          caption = "Response latency model estimates",
          escape = FALSE,
          note = "Degrees of freedom are 2747 for lexical decision tasks and 2753 for letter search tasks.",
          col.names = c("Task", "Predictor", "$b$", "$SE$", "$t$", "$p$"))

```

```{r RT-graph, echo=FALSE, fig.cap = "Response latency data for the lexical decision and letter search tasks for each stimuli type.", fig.height=6, fig.width=6}

noout$type2 = factor(noout$type, 
                     levels = c("Associative", "Nonword", "Semantic", "Unrelated"))

rtplot = ggplot(noout, aes(task, RT, fill = type2)) + 
  stat_summary(fun.y = mean,
               geom = "bar",
               position = "dodge") +
  stat_summary(fun.data = mean_cl_normal,
               geom = "errorbar", 
               position = position_dodge(width = 0.90),
               width = .2) +
  cleanup2 +
  xlab("Task Type") +
  ylab("Response Latency") + 
  scale_fill_manual(name = "Stimuli Type", 
                    values = c('gray20','gray40','gray60','gray80'))
rtplot
```

#Discussion

  These experiments were designed to explore the differences between N400 activation in the brain following presentation of semantic-only, associative-only, and unrelated word pairs in priming tasks. The N400 data presented a picture of semantic and associative attentuation in comparison to nonword and unrelated word pair stimuli for the lexical decision task. In contrast, the letter search task showed larger area under the curve results for the nonword stimuli, but no differences were found in the other stimuli pairs. The task data somewhat contradicted these results, as priming was found for associative word pairs only in the lexical decision task, while the letter search task showed both associative and semantic priming. It is possible that the task data were mixed because of the stimuli chosen, even though these were controlled as best as possible with avaliable semantic and associative databases. As @Hutchison2003 points out, associatively related items often tend to be semantically related, and norming tasks may miss some associations due to sampling. 
  
  Additionally, as seen in Figure \@ref(fig:RT-graph), the response latencies in this study are long, especially compared to the typical values found in the English Lexicon Project [@Balota2007]. The longer response times are likely due to task demands switching between lexical decision and letter search tasks, and these results are similar to @Smith2001. We did not replicate their findings for the letter search task, as we found both semantic and associative priming. These differences could be due to stimuli, prime type (i.e., our participants judged both target and prime), or SOA, as potentially their results only replicate at quick SOAs focused on semantic word pairs. Our experiment does expand their study by using database normed stimuli, while also expanding to semantic and associative stimuli. 
  
  These results suggest a mix of automatic and controlled processing during a demanding task-switching experiment. Although @Deacon2000 and @Deacon2004 point to potential issues of the N400 and automaticity versus controlled processing, our results may indicate the automatic processing of primed words over unrelated and nonwords when the focus is on the word reading level (i.e., the lexical decision task). In the letter search task, the focus on the orthographic or letter level may impeed the automatic processing, as viewed through an Interactive Activation model of word reading [@McClelland1981; @Rumelhart1982]. There are sometimes dissociations between the N400 and response latency measures. The use of the N400 can therefore be seen as an especially relevant dependent measure for the reason that components can only partially be a reflection of semantic processes relating to response latencies [@Kutas2011]. 
  
  Given the relatively long response latencies in our study, the task performance results may reflect controlled processing, especially post lexical matching [@Neely1991]. To date, research has focused on semantic priming and its automaticity without many controls for associative relationships embedded in word pairs. Therefore, our study does expand the smaller literature that focuses on separating these priming effects [@Buchanan2010; @XavierAlario2000; @Chiarello1990; @Perea1997]. Our current study has supported findings by @Mari-Beffa2005, who showed activation during letter search tasks, along with the many studies on automatic activation during masked priming [@Deacon2000;@Kiefer2002]. Additionally, the Semantic Priming Project has illustrated that priming is extremely variable across stimuli ranging from decreases of 200+ ms to *increases* of over 300 ms with an average priming effect of ~25 ms [@Hutchison2013]. 
  
  Limitations do exist within these experiments. A larger sample size would increase the power coefficient of the findings, and this study's sample size was selected due to the convenience sampling and time demands for an undergraduate thesis project. Future studies should focus on the extent of priming in semantic word pairs during a letter search task, which is a controversial topic within the literature. Since our study limited relatedness to associations or semantics, upcoming experiments could examine the interaction between word relationship type of N400 attenuation. @Kreher2006 have shown that N400 waveform differences can be attributed to different strengths of semantic relatedness in a linear fashion. With more exploration into the exact priming nature of associations and semantics, we may begin to discover their cognitive mechanisms.

\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
